<!doctype html>
 
<html class="no-js" lang="en" >  
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="chrome=1"> 
	<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
	<link rel="icon" type="image/x-icon" href="favicon.ico">
	<link rel="Bookmark" type="image/x-icon" href="favicon.ico">
	<link rel="apple-touch-icon" type="image/x-icon" href="favicon.png">
	<link rel="stylesheet" href="css/styles.css">  
	
	<title>Homepage of Huazhu FU</title>
	
</head>

<body> 
	<div id="wrapper"> 
		<section>
			<table border="0" id="table1" width="100%">
				<tbody>
					<tr> 
						<td>
							<h1> <a id="top-page" class="anchor" href="#top-page" aria-hidden="true"><span
								class="octicon octicon-link"></span></a>
								Huazhu FU</h1>
							<p>
								<b>Principal Scientist</b><br>
								<em> Institute of High Performance Computing (IHPC)<br>
								Agency for Science, Technology and Research (A*STAR), Singapore. </em>
							</p>
							<p>
								<b>Email:</b>  
								<em>hzfu(AT)ieee(DOT)org</em> 
							</p> 

							
						</td>  
						
						<!-- <td >
							<p align="left">
								<img border="0" src="sub_img/photo_hz.jpg" width="200">
							</p>

						</td>  -->
					</tr>
				</tbody>
			</table>

			<b>
				<a href="#new-page">[<ud>Recent News</ud>]</a> 
				- 
				<a href="#Services-page">[<ud>Professional Activities</ud>]</a> 
				- 
				<a href="#Highlighted-Paper-pages">[<ud>Highlighted Publications</ud>]</a> 
				- 
				<a href="#awards-pages">[<ud>Recognitions & Awards</ud>]</a>   
				- 
				<a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<b><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</b>]</a> 
				
			</b>
			<hr /> 

			<strong>My research focuses on:</strong>  
			<ul>
				<li> <strong>AI for Healthcare:</strong>
					Medical Image Analysis, ‚≠ê Medical Vision-Language Model, ‚≠ê Medical Foundation Model. 
				</li>
				<li> <strong>Trustworthy AI:</strong> 
					Federated Learning, ‚≠ê Uncertainty Estimation,  ‚≠ê Visual Grounding. 
				</li> 
			</ul>
				

			<hr />

			<!-- Section: Open position 

				<table border="0" id="table1" width="100%">
				<tbody>
			<tr> 
				<td style="width:15%">
					<p align="left">
						<img border="0" src="sub_img/open-position-logo.png" width="120">
					</p>
				</td>
			
				<td > 
					 <li>
						<b><hl_color>Scientist Position:</hl_color></b>
						We have several <a href="./job_poster.html" target="_blank"><b>Research Scientist Positions in AI for Healthcare and Trustworthy AI</b></a> available at IHPC, A*STAR, Singapore.  			  
					<li>
						<b><hl_color>PhD Scholarships:</hl_color></b>
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa" target="_blank"><b>Singapore International Graduate Award (SINGA) Scholarship</b></a> supports international students who wish to pursue their PhDs, collaborated with Singapore Universities (NUS, NTU, and SUTD). 
					</li>  
					<li>
						<b><hl_color>Visiting PhD Student:</hl_color></b> 
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-research-attachment-programme" target="_blank"><b>A*STAR Research Attachment Programme (ARAP)</b></a> supports PhD students from overseas universities to spend a minimum of one to a maximum of two years at A*STAR Research Institutes under the joint supervision.
					</li> 
					<li>
						<b><hl_color>Chinese PhD Student:</hl_color></b> 
						We are also looking for visiting PhD students from China to spend a minimum of one year at IHPC, under <a href="https://www.csc.edu.cn/chuguo" target="_blank"><b>Chinese CSC Scholarship</b></a>. 
						<em><hl_color>Requirements:</hl_color> Due to the limited headcount, we have to narrow down the list as: <strong>1)</strong> Having at least <strong>TWO</strong> first-author publications at top-tier conferences (e.g., CVPR, ICCV, ICLR) or journals (e.g., TPAMI, IJCV, TMI, TIP).
						<strong>2)</strong> Familiarity with one or more of the following will be viewed favorable: <strong>efficient learning</strong> (e.g., semi/self/weakly supervised learning, and domain adaptation), <strong>large foundation model</strong> (e.g., vision-language model, and generative model), and <strong>trustworthy AI</strong> (e.g., federated learning, uncertainty estimation, and explainable AI). </em>  
					</li> 
			
				</td> 
			</tr>
			</tbody>
			</table>
			<hr/>  
		-->
 
			<!-- Section: Recent News: -->
			
			<h3>
				<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
				<img src="sub_img/news-logo.gif" style="height:42px;"> Recent News:
			</h3>

			<div style="height:500px; overflow-x:hidden;">
			<ul>   

				<li>[<hl_color>Call For Paper</hl_color>] <strong>IEEE JBHI Special Issue</strong> on: "Next Generation AI for Eye Healthcare ‚Äî From Bench to Bedside" <a href="https://www.embs.org/jbhi/wp-content/uploads/sites/18/2025/09/JBHI_Next-Generation-AI-for-Eye-Healthcare-%E2%80%94-From-Bench-to-Bedside_SI_v1.pdf" target="_blank">[Link]</a> (Submission Deadline: 31 March 2026)
				</li> 

				<br>  


				<li>[10/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>:<br>  
					<a href="https://doi.org/10.1109/TMI.2025.3621452" target="_blank"><i>"Uncertainty-guided Prototype Reliability Enhancement Network for Few-Shot Medical Image Segmentation"</i></a>
				</li>
				<li>[09/2025] One paper accepted by <hl_color>IEEE RBME</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2505.10993" target="_blank"><i>"Content Generation Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges"</i></a>
				</li>
				<li>
					[09/2025] ü§ñ Our PraNet paper received the <hl_color>Young Scientist Publication Impact Award</hl_color> in MICCAI 2025! <br>  
					<a href="https://arxiv.org/abs/2006.11392" target="_blank"><i>"PraNet: Parallel Reverse Attention Network for Polyp Segmentation"</i></a>
				</li>
				<li>[09/2025] One paper accepted by <hl_color>NeurIPS 2025</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2505.17982" target="_blank"><i>"Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling"</i></a>
				</li>
				<li>[09/2025] One paper accepted by <hl_color>npj Digital Medicine</hl_color>:<br>  
					<a href="https://www.nature.com/articles/s41746-025-01988-2" target="_blank"><i>"A Deep Learning Based Automatic Report Generator for Retinal Optical Coherence Tomography Images"</i></a>
				</li>
				<li>[08/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2508.10299" target="_blank"><i>"Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"</i></a>
				</li>
				<li>[08/2025] One paper accepted by <hl_color>IEEE TPAMI</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2404.06798" target="_blank"><i>"Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding"</i></a>
				</li>
				<li>[08/2025] One paper accepted by <hl_color>IEEE TPAMI</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2508.03197" target="_blank"><i>"Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network"</i></a>
				</li>
				<li>[08/2025] One paper accepted by <hl_color>ACMMM 2025 Datasets Track</hl_color>:<br> 
					<a href="https://arxiv.org/abs/2506.17939" target="_blank"><i>"GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning"</i></a>
				</li>  
				<li>[08/2025] One paper accepted by <hl_color>IEEE TIP</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2404.06798" target="_blank"><i>"Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation"</i></a>
				</li>
				<li>[07/2025] One paper accepted by <hl_color>Nature Medicine</hl_color>:<br>  
					<a href="https://www.nature.com/articles/s41591-025-03900-7" target="_blank"><i>"An eyecare foundation model for clinical assistance: a randomized controlled trial"</i></a>
				</li>
				<li>[06/2025] Two papers accepted by <hl_color>ICCV 2025</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2411.16778" target="_blank"><i>"GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis"</i></a> 
					<br>  
					<a href="https://arxiv.org/abs/2403.18878" target="_blank"><i>"Teaching AI the Anatomy Behind the Scan: Addressing Anatomical Flaws in Medical Image Segmentation with Learnable Prior"</i></a>
				</li>
				<li>[06/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2409.04356" target="_blank"><i>"Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model"</i></a>
				</li>
				<li>[06/2025] Eight papers accepted by <hl_color>MICCAI 2025</hl_color>.</li>
				<li>[05/2025] One paper accepted by <hl_color>Nature Communications</hl_color>:<br>  
					<a href="https://www.nature.com/articles/s41467-025-60577-9" target="_blank"><i>"Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model"</i></a>
				</li>
				<li>[05/2025] One paper accepted by <hl_color>MedIA</hl_color>:<br> 
					<a href="https://arxiv.org/abs/2505.22522" target="_blank"><i>"PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation"</i></a>
				</li>
				<li>[03/2025] One paper accepted by <hl_color>MedIA</hl_color>:<br>  
					<a href="https://arxiv.org/abs/2304.12620" target="_blank"><i>"Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"</i></a>
				</li>
				<li>[03/2025] One paper accepted by <hl_color>MedIA</hl_color>: <br> 
					<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841525000611" target="_blank"><i>"Beyond the eye: A relational model for early dementia detection using retinal OCTA images"</i></a>
				</li>
				<li>[02/2025] Three papers accepted by <hl_color>CVPR 2025</hl_color>:<br>
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MExD_An_Expert-Infused_Diffusion_Model_for_Whole-Slide_Image_Classification_CVPR_2025_paper.html" target="_blank"><i>"MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification"</i></a> 
					<br> 
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Vision-Language_Model_IP_Protection_via_Prompt-based_Learning_CVPR_2025_paper.html" target="_blank"><i>"Vision-Language Model IP Protection via Prompt-based Learning"</i></a> 
					<br> 
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_A_Simple_Data_Augmentation_for_Feature_Distribution_Skewed_Federated_Learning_CVPR_2025_paper.html" target="_blank"><i>"A Simple Data Augmentation for Feature Distribution Skewed Federated Learning"</i></a> 
				</li>
				<li>[01/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10887048" target="_blank"><i>"Masked Vascular Structure Completion in Retinal Images"</i></a>
				</li>
				<li>[01/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>: <br>
					<a href="https://ieeexplore.ieee.org/document/10855574" target="_blank"><i>"Uncertainty-Driven Edge Prompt Generation Network for Medical Image Segmentation"</i></a>
				</li>
				<li>[01/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10843248" target="_blank"><i>"Multi-view Test-time Adaptation for Semantic Segmentation in Clinical Cataract Surgery"</i></a>
				</li>
				<li>[01/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>: <br> 
					<a href="https://arxiv.org/abs/2412.13742" target="_blank"><i>"Learnable Prompting SAM-induced Knowledge Distillation for Semi-supervised Medical Image Segmentation"</i></a>
				</li>
				<li>[01/2025] One paper accepted by <hl_color>IEEE TMI</hl_color>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10843287" target="_blank"><i>"DiffMIC-v2: Medical Image Classification via Improved Diffusion Network"</i></a>
				</li>

			</ul>

		</div>
 
			<hr />

			<h3>
				<a id="Services-page" class="anchor" href="#Services-page" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
						<img src="sub_img/activity-logo.gif" style="height:42px;"> Professional Activities:
			</h3>
			
			<ul> 
					<ul>
						<!-- <li> IEEE Senior Member. </li> -->
						<li> 
							<strong>Co-Chair</strong> of ‚ÄúStudents & Young Professionals‚Äù Subcommittee in the IEEE Signal Processing Society (SPS) Technical Committee on Bio Imaging and Signal Processing (<a href="https://signalprocessingsociety.org/community-involvement/bio-imaging-and-signal-processing/bisp-tc-home" target="_blank"><strong>BISP</strong></a>).
						</li>
						<li>
							<strong>Member</strong> of the IEEE Engineering in Medicine and Biology Society (EMBS) Technical Committee on Biomedical Imaging and Image Processing (<a href="https://www.embs.org/biip/" target="_blank"><strong>BIIP</strong></a>).

						</li>
					</ul>  

				<li><strong>Associate Editor:</strong>
					<ul>
						<li> IEEE Transactions on Medical Imaging (<strong>IEEE TMI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Neural Networks and Learning Systems (<strong>IEEE TNNLS</strong>), 2022 - present. </li>
						<li> IEEE Journal of Biomedical and Health Informatics (<strong>IEEE JBHI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Artificial Intelligence (<strong>IEEE TAI</strong>), 2023 - present. </li>
						<li> Pattern Recognition (<strong>PR</strong>), 2024 - present. </li>
						<!-- <li> Scientific Reports, 2021 - present. </li> -->
						<li> Visual Intelligence, 2024 - present. </li>
						<li> Meta-Radiology, 2023 - present. </li>
						<!-- <li> IEEE Access, 2018 - present. </li> -->
					</ul>
				</li>  

				<li><strong>Challenge Organizer:</strong>
					<ul>
						<li>
							"<strong>GAVE</strong>: Generalized Analysis of Vessels in Eye" with the MICCAI 2025.  
							[<a href="https://aistudio.baidu.com/competition/detail/1315/0/introduction" target="_blank">Link</a>] 
						</li>
						<li>
							"<strong>STAGE2</strong>: 2nd Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2024.  
							[<a href="https://aistudio.baidu.com/aistudio/competition/detail/1167" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>ATLAS</strong>: A Tumor and Liver Automatic Segmentation" with the MICCAI 2023. 
							[<a href="https://atlas-challenge.u-bourgogne.fr" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>STAGE</strong>: Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2023. 
							[<a href="https://aistudio.baidu.com/aistudio/competition/detail/1167" target="_blank">Link</a>]
						</li>

						<li>
							"<strong>GOALS</strong>: Glaucoma Oct Analysis and Layer Segmentation" with the MICCAI 2022.
							[<a href="https://aistudio.baidu.com/competition/detail/783/0/introduction" target="_blank">Link</a>] 
						</li>
						<li>
							"<strong>GAMMA</strong>: Glaucoma Grading from Multi-Modality Images Challenge" with the MICCAI 2021.
							[<a href="https://gamma.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2202.06511" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>REFUGE2</strong>: 2nd Retinal Fundus Glaucoma Challenge" with the MICCAI 2020.
							[<a href="https://refuge.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.08994" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>ADAM</strong>: Automatic Detection challenge on Age-related Macular degeneration" with the ISBI 2020.
							[<a href="https://amd.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.07983" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>AGE</strong>: Angle closure Glaucoma Evaluation Challenge" with the MICCAI 2019.
							[<a href="https://age.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2005.02258" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>PALM</strong>: PathologicAL Myopia detection from retinal images" with the ISBI 2019. [<a href="https://palm.grand-challenge.org" target="_blank">Link</a>] 
							[<a href="https://doi.org/10.1038/s41597-024-02911-2" target="_blank">Data Summary</a>]
						</li>
						<li>
							"<strong>REFUGE</strong>: Retinal Fundus Glaucoma Challenge" with the MICCAI 2018. 
							[<a href="https://refuge.grand-challenge.org/" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/1910.03667" target="_blank">Summary Paper</a>]
						</li>
					</ul>
				</li>
			</ul> 

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />
  
			<h3>
				<a id="Highlighted-Paper-pages" class="anchor" href="#Highlighted-Paper-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
				<img src="sub_img/paper-logo.gif" style="height:42px;"> Highlighted Publications: 
			</h3>

			<strong>Full publication list:</strong>
				<a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<strong><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</strong>]</a>  

			<table border="0" width="100%"> 
				<tr>
					<th style="width:10%"></th>
					<th></th> 
				</tr>
				

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/npj_Digital_Medicine_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41746-025-01988-2" target="_blank"><strong>"A deep learning based automatic report generator for retinal optical coherence tomography images"</strong></a>, <br>
					
							X. Chen, <strong>H. Fu</strong>, J. Wang, ......, C.P. Pang, and H. Chen, <br>
					
							<i><strong>npj Digital Medicine</strong>, 2025.</i> 
							<a href="https://github.com/Poizon1213/Retinal_OCT_Report_Automatic_Generation" target="_blank">[Code]</a>  
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/ieee-rbme-logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2505.10993" target="_blank"><strong>"Content Generation Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges"</strong></a>,   <br>
							Y. Zhang, X. Zhang, X. Qi, X. Wu, F. Chen, G. Yang, and <strong>H. Fu</strong>,<br>  
							<i><strong>IEEE Reviews in Biomedical Engineering</strong>, 2025.</i>  
							<a href="https://github.com/yuanzhang7/Awesome-Generative-Models-in-Pathology" target="_blank">[Homepage]</a> 
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-025-60577-9" target="_blank"><strong>"Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model"</strong></a>,   <br>
							M. Wang, T. Lin, A. Lin, K. Yu, ......, C.-Y. Cheng, H. Chen, and <strong>H. Fu</strong>,<br>  
							<i><strong>Nature Communications</strong>, 2025.</i>  
							<a href="https://github.com/LooKing9218/RetiZero" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Medicine_Logo.png" width="100"></p>
					</td>  
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41591-025-03900-7" target="_blank"><strong>"An Eyecare Foundation Model for Clinical Assistance: a Randomized Controlled Trial"</strong></a>,   <br>
							Y. Wu, B. Qian, T. Li, Y. Qin,......, <strong>H. Fu</strong>, ......, T. Y. Wong, and B. Sheng, <br>  
							<i><strong>Nature Medicine</strong>, 2025.</i> 
							<a href="https://github.com/eyefm/EyeFM" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr> 

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2404.06798" target="_blank"><strong>"Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding"</strong></a>, <br>
							K. Zou, Y. Bai, B. Liu, Y. Chen, Z. Chen, Y. Zhou, X. Yuan, M. Wang, X. Shen, X. Cao, Y. C. Tham, and, <strong>H. Fu</strong>, <br>
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2025.</i> 
							<a href="https://github.com/Cocofeat/uMedGround" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2508.03197" target="_blank"><strong>"Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network"</strong></a>, <br>
							T. Chen, D. Zhang, D. Chen, <strong>H. Fu</strong>, K. Jin, S. Wang, L. D. Cohen, Y. Zhao, Q. Yi, and J. Zhang, <br>
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2025.</i>
							<a href="https://github.com/jiongzhang-john/CNVSeg-Dataset" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Cell-Reports-Medicine-Logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://doi.org/10.1016/j.xcrm.2024.101876" target="_blank"><strong>"Enhancing Al Reliability: A Foundation Model with Uncertainty Estimation for Optical Coherence Tomography based Retinal Diseases Diagnosis"</strong></a>, <br>
							Y. Peng, A. Lin, M. Wang, T. Lin, ......, C.-Y. Cheng, <strong>H. Fu</strong>, and H. Chen, <br> 
							<i><strong>Cell Reports Medicine</strong>, 2024.</i> <a href="https://github.com/yuanyuanpeng0129/FMUE" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2408.13161" target="_blank"><strong>"Say No to Freeloader: Protecting Intellectual Property of Your Deep Model"</strong></a>, <br>
							L. Wang, M. Wang, <strong>H. Fu</strong>, and D. Zhang, <br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2024.</i> <a href="https://github.com/LyWang12/CUPI-Domain" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2205.12857" target="_blank"><strong>"Structure Unbiased Adversarial Model for Medical Image Translation"</strong></a>,<br>
					
							T. Zhang, S. Zheng, J. Cheng, X. Jia, J. Bartlett, X. Cheng, Z. Qiu, <strong>H. Fu</strong>, J. Liu, A. Leonardis, and J. Duan,<br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2024.</i> <a href="https://traceable-translation.github.io/" target="_blank">[Online Demo]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/npj_Digital_Medicine_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41746-024-01292-5" target="_blank"><strong>"Early Detection of Dementia through Retinal Imaging and Trustworthy AI"</strong></a>, <br>
					
							J. Hao, W. Kwapong, T. Shen, <strong>H. Fu</strong>, ......,  S. Zhang, H. Qi, and Y. Zhao, <br>
					
							<i><strong>npj Digital Medicine</strong>, 2024.</i> 
					
							<a href="https://github.com/iMED-Lab/Eye-AD" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-023-42444-7" target="_blank"><strong>"Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification"</strong></a>,   <br>
					
							M. Wang, T. Lin, L. Wang, A. Lin, ......, X. Chen, H. Chen, and <strong>H. Fu</strong>,<br>
					
							<i><strong>Nature Communications</strong>, 2023.</i> 
					
							<a href="https://github.com/LooKing9218/UIOS" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Machine_Intelligence_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s42256-023-00652-2" target="_blank"><strong>"Federated Benchmarking of Medical Artificial Intelligence with MedPerf"</strong></a>, <br>
					
							A. Karargyris, R. Umeton, M. J. Sheller, ......,  <strong>H. Fu</strong>, ......, J. M. Johnson, S. Bakas, and P. Mattson,<br>
					
							<i><strong>Nature Machine Intelligence</strong>, 2023.</i> 
					
							<a href="https://github.com/mlcommons/medperf" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-023-36796-3" target="_blank"><strong>"Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST"</strong></a>,<br>
					
							Y. Long, K. S. Ang, M. Li, ......, <strong>H. Fu</strong>, ......, L. Liu, and J. Chen,<br>
					
							<i><strong>Nature Communications</strong>, 2023.</i> 
					
							<a href="https://github.com/JinmiaoChenLab/DeepST" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr> 


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2205.15469" target="_blank"><strong>"GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector"</strong></a>,<br>
					
							P. Zheng, <strong>H. Fu</strong>, D.-P. Fan, Q. Fan, J. Qin, Y.-W. Tai, C.-K. Tang, and L. V. Gool,<br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i>
					
							<a href="https://github.com/ZhengPeng7/GCoNet_plus" target="_blank">[Code]</a>

						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2204.11423" target="_blank"><strong>"Trusted Multi-View Classification with Dynamic Evidential Fusion"</strong></a>,<br>
					
							Z. Han, C. Zhang, <strong>H. Fu</strong>, and J. T. Zhou,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i> 
					
							<a href="https://github.com/hanmenghan/TMC" target="_blank">[Code]</a>
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2202.04861" target="_blank"><strong>"Consistency and Diversity induced Human Motion Segmentation"</strong></a>,<br>
					
							T. Zhou, <strong>H. Fu</strong>, C. Gong, L. Shao, F. Porikli, H. Ling, and J. Shen,</em><br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i>  
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2007.03380" target="_blank"><strong>"Re-thinking Co-Salient Object Detection"</strong></a>, <br>
					
							D.-P. Fan, T. Li, Z. Lin, G.-P. Ji, D. Zhang, M.-M. Cheng, <strong>H. Fu</strong>, and J. Shen, <br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>  
					
							<a href="https://github.com/DengPingFan/CoEGNet" target="_blank">[Code]</a> 
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/1904.09146" target="_blank"><strong>"Salient Object Detection in the Deep Learning Era: An In-Depth Survey",</strong></a><br>
					
							W. Wang, Q. Lai, <strong>H. Fu</strong>, J. Shen, H. Ling, and R. Yang,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>
					
							<a href="https://github.com/wenguanwang/SODsurvey" target="_blank">[Project]</a> 
					
						</p>

					</td> 
				</tr>



				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2011.06170" target="_blank"><strong>"Deep Partial Multi-View Learning"</strong></a>,<br>
					
							C. Zhang, Y. Cui, Z. Han, J. T. Zhou, <strong>H. Fu</strong>, and Q. Hu,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>
					
							<a href="https://github.com/hanmenghan/CPM_Nets" target="_blank">[Code]</a> 
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://www.researchgate.net/publication/328479701_Generalized_Latent_Multi-View_Subspace_Clustering" target="_blank"><strong>"Generalized Latent Multi-view Subspace Clustering"</strong></a>,<br>
					
							C. Zhang, <strong>H. Fu</strong>, Q. Hu, X. Cao, Y. Xie, D. Tao, and D. Xu, <br> 
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2020.</i> 
					
							<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/LMSC_CVPR2017_Zhang.rar">[Code]</a> 
					
							<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/ORL_mtv.rar">[Data]</a>
				</li>    
					
						</p>

					</td> 
				</tr>

			</table>   



			<div align="right">
			<a href="#top-page">[<b>Back to top</b>]</a>
			</div>
			<hr />

			<h3>
			<a id="awards-pages" class="anchor" href="#awards-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
			<img src="sub_img/award-logo.gif" style="height:42px;"> Recognitions & Awards: 
			</h3>

			<ul> 

			<li>
				2025: <strong>Young Scientist Publication Impact Award</strong> in MICCAI.
				<br><i>By "PraNet: Parallel Reverse Attention Network for Polyp Segmentation".</i>
			</li> 

			<li> 
				2020-2025: <strong>World's Top 2% Scientists List</strong>, by Stanford/Elsevier (in category Artificial Intelligence & Image Processing).
				<a href="https://topresearcherslist.com" target="_blank">[Link]</a>
			</li>

			<li> 
				2024: <strong>Highly Cited Researcher</strong>, in Web of Science (in the field of Cross-Field) by Clarivate. 
				<a href="https://clarivate.com/highly-cited-researchers/" target="_blank">[Link]</a>
			</li>

			<li>
				2024: <strong>First Place</strong> in <a href="https://2024.asiateleophth.org/big-data-competition/" target="_blank">"Big Data Competition"</a> in APTOS-APOIS 2024.
			</li>

			<li>
				2024: <strong>Best Paper Award</strong> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI.
				<br><i>By "Enhancing Large Foundation Models to Identify Fundus Diseases Based on Contrastive Enhanced Low-Rank Adaptation Prompt".</i> 
			</li>

			

			<li>
				2024: <strong>Collaborative Paper Award</strong> by Biomedical Research Council (BMRC), A*STAR. 
				<br><i>By "Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST".</i>
			</li>
				
			<li>
				2024: <strong>Outstanding Presentation</strong> under the Computing, Data and Digital Sciences track at A*STAR CDF Day, 2024.
			</li>

			<li>
				2023: <strong>Honorable Mention Award</strong> by Computational Visual Media Journal. <a href="https://link.springer.com/journal/41095/updates/26965690" target="_blank">[Link]</a>
				<br> <i>By "Specificity-preserving RGB-D saliency detection".</i> 
				
			</li> 

			<li>
				2023: <strong>Achievement Award</strong>  of  2023 TOP 100 Benchmarks & Evaluation by International Open Benchmark Council.
			</li>

			<li>
				2023: <strong>Best Paper Award</strong> of  Distributed, Collaborative and Federated Learning (DeCAF) Workshop in MICCAI. 
				<br> <i>By "Federated model aggregation via self-supervised priors for highly imbalanced medical image classification".</i>
			</li>

			<li>
				2022: <strong>Best Paper Award</strong> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI. 
				<br> <i>By "Localizing anatomical landmarks in ocular images using zoom-in attentive networks".</i>
			</li>
				
			<li>
				2022: <strong>Best Paper Runner-up Award</strong> of Resource-Efficient Medical Image Analysis (REMIA) Workshop in MICCAI. 
				<br> <i>By "Facing annotation redundancy: Oct layer segmentation with only 10 annotated pixels per layer".</i>
			</li>

			<li>
				2021: <strong>Best Paper Award</strong> in IEEE International Conference on Multimedia & Expo (ICME). 
				<a href="http://2021.ieeeicme.org/2021.ieeeicme.org/best_paper_awards.html" target="_blank">[Link]</a>
				<br> <i>By "Cross-view equivariant auto-encoder".</i>
			</li> 

			<li>
				2021: <strong>Finalist</strong> of the Young Scientist Publication Impact Award in MICCAI.
				<br> <i>By "DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field".</i>
			</li>
				
			<li>
				2021: <strong>Most Influential Paper (Application) Award</strong> in Jittor Developer Conference.
				<br> <i>By "PraNet: Parallel Reverse Attention Network for Polyp Segmentation".</i>
			</li>  

			
			</ul>
			
			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			</div> 

			<hr />
			
			<br>

		</section>

	</div>
</body>

</html>
