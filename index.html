<!doctype html>
 
<html class="no-js" lang="en" >  
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="chrome=1"> 
	<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
	<link rel="icon" type="image/x-icon" href="favicon.ico">
	<link rel="Bookmark" type="image/x-icon" href="favicon.ico">
	<link rel="apple-touch-icon" type="image/x-icon" href="favicon.png">
	<link rel="stylesheet" href="css/styles.css">  
	
	<title>Homepage of Huazhu FU</title>
	
</head>

<body> 
	<div id="wrapper"> 
		<section>
			<table border="0" id="table1" width="100%">
				<tbody>
					<tr> 
						<td>
							<h1> <a id="top-page" class="anchor" href="#top-page" aria-hidden="true"><span
								class="octicon octicon-link"></span></a>
								Huazhu FU</h1>
							<p>
								<b>Principal Scientist</b><br>
								<em> Institute of High Performance Computing (IHPC)<br>
								Agency for Science, Technology and Research (A*STAR), Singapore. </em>
							</p>
							<p>
								<b>Email:</b>  
								<em>hzfu(AT)ieee(DOT)org</em> 
							</p> 

							
						</td>  
						
						<!-- <td >
							<p align="left">
								<img border="0" src="sub_img/photo_hz.jpg" width="200">
							</p>

						</td>  -->
					</tr>
				</tbody>
			</table>

			<b>
				<a href="#new-page">[<ud>Recent News</ud>]</a> 
				- <a href="#Services-page">[<ud>Professional Activities</ud>]</a> 
				- <a href="#Highlighted-Paper-pages">[<ud>Highlighted Publications</ud>]</a> 
				- <a href="#awards-pages">[<ud>Recognitions & Awards</ud>]</a>   
				- <a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<b><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</b>]</a> 
				
			</b>
			<hr />

			I am  a principal scientist at IHPC, A*STAR.  My research focuses on medical image analysis, AI for healthcare, and trustworthy AI.
			
			<br> 


			<strong>Major focuses on:</strong>   
			<li> <strong>Computer Vision:</strong>  Foreground Detection, Image Segmentation, Image Restoration. 
			</li> 
				<li> <strong>AI for Healthcare:</strong>
					Medical Image Analysis, ⚡Medical Vision-Language Model, ⚡Medical Foundation Model. 
				</li>
				<li> <strong>Trustworthy AI:</strong> 
					⚡Uncertainty Estimation, ⚡Federated Learning. 
				</li> 
				

			<hr />

			<!-- Section: Open position 
				
				
				<table border="0" id="table1" width="100%">
				<tbody>
			<tr> 
				<td style="width:15%">
					<p align="left">
						<img border="0" src="sub_img/open-position-logo.png" width="120">
					</p>
				</td>
			
				<td > 
					 <li>
						<b><hl_color>Scientist Position:</hl_color></b>
						We have several <a href="./job_poster.html" target="_blank"><b>Research Scientist Positions in AI for Healthcare and Trustworthy AI</b></a> available at IHPC, A*STAR, Singapore.  			  
					<li>
						<b><hl_color>PhD Scholarships:</hl_color></b>
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa" target="_blank"><b>Singapore International Graduate Award (SINGA) Scholarship</b></a> supports international students who wish to pursue their PhDs, collaborated with Singapore Universities (NUS, NTU, and SUTD). 
					</li>  
					<li>
						<b><hl_color>Visiting PhD Student:</hl_color></b> 
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-research-attachment-programme" target="_blank"><b>A*STAR Research Attachment Programme (ARAP)</b></a> supports PhD students from overseas universities to spend a minimum of one to a maximum of two years at A*STAR Research Institutes under the joint supervision.
					</li> 
					<li>
						<b><hl_color>Chinese PhD Student:</hl_color></b> 
						We are also looking for visiting PhD students from China to spend a minimum of one year at IHPC, under <a href="https://www.csc.edu.cn/chuguo" target="_blank"><b>Chinese CSC Scholarship</b></a>. 
						<em><hl_color>Requirements:</hl_color> Due to the limited headcount, we have to narrow down the list as: <strong>1)</strong> Having at least <strong>TWO</strong> first-author publications at top-tier conferences (e.g., CVPR, ICCV, ICLR) or journals (e.g., TPAMI, IJCV, TMI, TIP).
						<strong>2)</strong> Familiarity with one or more of the following will be viewed favorable: <strong>efficient learning</strong> (e.g., semi/self/weakly supervised learning, and domain adaptation), <strong>large foundation model</strong> (e.g., vision-language model, and generative model), and <strong>trustworthy AI</strong> (e.g., federated learning, uncertainty estimation, and explainable AI). </em>  
					</li> 
			
				</td> 
			</tr>
			</tbody>
			</table>
			<hr/>  
		-->
 
			<!-- Section: Recent News: -->
			
			<h3>
				<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
				<img src="sub_img/news-logo.gif" style="height:42px;"> Recent News:
			</h3>

			<!-- <div style="height:300px; overflow-x:hidden;"> -->
			<ul>   

				<!-- <li>[<hl_color>Call For Paper</hl_color>] <strong>IEEE JBHI Special Issue</strong> on: "Revolutionizing Healthcare with LLMs: Breakthroughs in Decision Support, Patient Interaction, and Documentation" <a href="https://www.embs.org/jbhi/wp-content/uploads/sites/18/2024/12/JBHI_CFP_LLM_Healthcare.pdf" target="_blank">[Link]</a> (Submission Deadline: 20 April 2025)
				</li>

				<li>[<hl_color>Call For Paper</hl_color>] <strong>IEEE JBHI Special Issue</strong> on: "Synergizing Multi-modal Agents and Large Foundation Models for Healthcare" <a href="https://www.embs.org/jbhi/wp-content/uploads/sites/18/2025/01/Special-Issue-on-Synergizing-Multi-modal-Agents-and-Large-Foundation-Models-for-Healthcare.pdf" target="_blank">[Link]</a> (Submission Deadline: 31 May 2025)
				</li>

				<br> -->


				<li>[08/2025] One paper accepted by <strong>IEEE TMI</strong>:<br>  
					<a href="https://ieeexplore.ieee.org/document/11119645" target="_blank">"Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"</a>
				</li>
				<li>[08/2025] One paper accepted by <strong>IEEE TPAMI</strong>:<br>  
					<a href="https://arxiv.org/abs/2404.06798" target="_blank">"Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding"</a>
				</li>
				<li>[08/2025] One paper accepted by <strong>IEEE TPAMI</strong>:<br>  
					<a href="https://arxiv.org/abs/2508.03197" target="_blank">"Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network"</a>
				</li>
				<li>[08/2025] One paper accepted by <strong>ACMMM 2025 Datasets Track</strong>:<br> 
					<a href="https://arxiv.org/abs/2506.17939" target="_blank">"GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning"</a>
				</li>  
				<li>[08/2025] One paper accepted by <strong>IEEE TIP</strong>:<br>  
					"Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation"
				</li>
				<li>[07/2025] One paper accepted by <strong>Nature Medicine</strong>:<br>  
					"An eyecare foundation model for clinical assistance: a randomized controlled trial"
				</li>
				<li>[06/2025] Two papers accepted by <strong>ICCV 2025</strong>:<br>  
					<a href="https://arxiv.org/abs/2411.16778" target="_blank">"GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis"</a> 
					<br>  
					<a href="https://arxiv.org/abs/2403.18878" target="_blank">"Teaching AI the Anatomy Behind the Scan: Addressing Anatomical Flaws in Medical Image Segmentation with Learnable Prior"</a>
				</li>
				<li>[06/2025] One paper accepted by <strong>IEEE TMI</strong>:<br>  
					<a href="https://arxiv.org/abs/2409.04356" target="_blank">"Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model"</a>
				</li>
				<li>[06/2025] Eight papers accepted by <strong>MICCAI 2025</strong>.</li>
				<li>[05/2025] One paper accepted by <strong>Nature Communications</strong>:<br>  
					<a href="https://www.nature.com/articles/s41467-025-60577-9" target="_blank">"Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model"</a>
				</li>
				<li>[05/2025] One paper accepted by <strong>MedIA</strong>:<br> 
					<a href="https://arxiv.org/abs/2505.22522" target="_blank">"PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation"</a>
				</li>
				<li>[03/2025] One paper accepted by <strong>MedIA</strong>:<br>  
					<a href="https://arxiv.org/abs/2304.12620" target="_blank">"Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"</a>
				</li>
				<li>[03/2025] One paper accepted by <strong>MedIA</strong>: <br> 
					<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841525000611" target="_blank">"Beyond the eye: A relational model for early dementia detection using retinal OCTA images"</a>
				</li>
				<li>[02/2025] Three papers accepted by <strong>CVPR 2025</strong>:<br>
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MExD_An_Expert-Infused_Diffusion_Model_for_Whole-Slide_Image_Classification_CVPR_2025_paper.html" target="_blank">"MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification"</a> 
					<br> 
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Vision-Language_Model_IP_Protection_via_Prompt-based_Learning_CVPR_2025_paper.html" target="_blank">"Vision-Language Model IP Protection via Prompt-based Learning"</a> 
					<br> 
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_A_Simple_Data_Augmentation_for_Feature_Distribution_Skewed_Federated_Learning_CVPR_2025_paper.html" target="_blank">"A Simple Data Augmentation for Feature Distribution Skewed Federated Learning"</a> 
				</li>
				<li>[01/2025] One paper accepted by <strong>IEEE TMI</strong>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10887048" target="_blank">"Masked Vascular Structure Completion in Retinal Images"</a>
				</li>
				<li>[01/2025] One paper accepted by <strong>IEEE TMI</strong>: <br>
					<a href="https://ieeexplore.ieee.org/document/10855574" target="_blank">"Uncertainty-Driven Edge Prompt Generation Network for Medical Image Segmentation"</a>
				</li>
				<li>[01/2025] One paper accepted by <strong>IEEE TMI</strong>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10843248" target="_blank">"Multi-view Test-time Adaptation for Semantic Segmentation in Clinical Cataract Surgery"</a>
				</li>
				<li>[01/2025] One paper accepted by <strong>IEEE TMI</strong>: <br> 
					<a href="https://arxiv.org/abs/2412.13742" target="_blank">"Learnable Prompting SAM-induced Knowledge Distillation for Semi-supervised Medical Image Segmentation"</a>
				</li>
				<li>[01/2025] One paper accepted by <strong>IEEE TMI</strong>: <br> 
					<a href="https://ieeexplore.ieee.org/document/10843287" target="_blank">"DiffMIC-v2: Medical Image Classification via Improved Diffusion Network"</a>
				</li>

				<!-- 

				<li>------ <em>Happy New Year!</em> --------</li> 
				<li>[12/2024] One paper accepted by <strong>IEEE TIP</strong>: "VSR-Net: Vessel-like Structure Rehabilitation Network with Graph Clustering".</li>
				<li>[12/2024] We secured the <strong>First Place</strong> in <a href="https://2024.asiateleophth.org/big-data-competition/" target="_blank">"APTOS-APOIS Big Data Competition"</a> on generating optical coherence tomography (OCT) images from color fundus photography.</li>
				<li>[12/2024] One paper accepted by <strong>IEEE TMI</strong>: "Training-free image style alignment for domain shift on handheld ultrasound devices".</li>
				<li>[11/2024] One paper accepted by <strong>IEEE TMI</strong>: "Topicwise Separable Sentence Retrieval for Medical Report Generation".</li>
				<li>[11/2024] Happy to be recognized as <hl_color>"Highly Cited Researcher"</hl_color> by Web of Science (in the field of Cross-Field), Clarivate. </li>
				<li>[10/2024] One paper accepted by <strong>Cell Reports Medicine</strong>: "Enhancing Al Reliability: A Foundation Model with Uncertainty Estimation for Optical Coherence Tomography based Retinal Diseases Diagnosis".</li>
				<li>[10/2024] One paper accepted by <strong>IEEE TMI</strong>: "CoD-MIL: Chain-of-Diagnosis Prompting Multiple Instance Learning for Whole Slide Image Classification".</li>
				<li>[20/2024] Happy to receive the <hl_color>Best Paper Award</hl_color>  of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI, 2024. </li>
				<li>[10/2024] One paper accepted by <strong>npj Digital Medicine</strong>: "Early Detection of Dementia through Retinal Imaging and Trustworthy AI".</li>

				

				<li>[09/2024] Two papers accepted by <strong>NeurIPS 2024</strong>.</li>
				<li>[09/2024] One paper accepted by <strong>EMNLP 2024</strong>: "Self-Training Large Language and Vision Assistant for Medical Question Answering".</li>
				<li>[08/2024] One paper accepted by <strong>IEEE TPAMI</strong>: "Say No to Freeloader: Protecting Intellectual Property of Your Deep Model".</li>
				<li>[08/2024] One paper accepted by <strong>MedIA</strong>: "E2-MIL: An Explainable and Evidential Multiple Instance Learning Framework for Whole Slide Image Classification".</li>
				<li>[07/2024] One paper accepted by <strong>IEEE TPAMI</strong>: "Structure Unbiased Adversarial Model for Medical Image Translation".</li>
				<li>[06/2024] 11 papers accepted by <strong>MICCAI 2024</strong>.</li> 
				<li>[05/2024] One paper accepted by <strong>MedIA</strong>: "Confidence-aware multi-modality learning for eye disease screening".</li>
				
				<li>[04/2024] Our paper, "Specificity-preserving RGB-D saliency detection", received the <hl_color>Honorable Mention Award</hl_color> of Computational Visual Media Journal in 2023.</li>
				<li>[04/2024] One paper accepted by <strong>IJCV</strong>: "ViDSOD-100: A New Dataset and A Baseline Model for RGB-D Video Salient Object Detection".</li>
				<li>[03/2024] One paper accepted by <strong>IEEE TMI</strong>: "Diverse Data Generation for Retinal Layer Segmentation with Potential Structure Modelling".</li>
				<li>[03/2024] One paper accepted by <strong>IEEE TMI</strong>: "Instrument-tissue Interaction Detection Framework for Surgical Video Understanding".</li> 
				<li>[02/2024] Two papers accepted by <strong>CVPR 2024</strong>.</li>
				<li>[01/2024] One paper accepted by <strong>Genome Medicine</strong>: "Unsupervised spatially embedded deep representation of spatial transcriptomics".</li> 
				<li>[01/2024] One paper accepted by <strong>IEEE TMI</strong>: "Geometric Correspondence-Based Multimodal Learning for Ophthalmic Image Analysis".</li>
				<li>[01/2024] Our PALM challenge paper accepted by <strong>Scientific Data</strong>: "Open Fundus Photograph Dataset with Pathologic Myopia Recognition and Anatomical Structure Annotation"</li>

				
				<li>[12/2023] One paper accepted by <strong>IEEE TMI</strong>: "Bilateral Supervision Network for Semi-supervised Medical Image Segmentation".</li>
				<li>[12/2023] One paper accepted by <strong>AAAI</strong> 2024.</li>
				<li>[12/2023] Our iChallenge datasets receive the <hl_color>Achievement Award</hl_color>  of  2023 TOP 100 Benchmarks & Evaluation by International Open Benchmark Council.</li>
				<li>[11/2023] One paper accepted by <strong>IEEE TMI</strong>: "Enhancing and Adapting in the Clinic: Source-free Unsupervised Domain Adaptation for Medical Image Enhancement".</li>
				<li>[11/2023] One paper accepted by <strong>IEEE TNNLS</strong>: "Federated Noisy Client Learning".</li>
				<li>[11/2023] One paper accepted by <strong>IEEE TMI</strong>: "Edge-guided Contrastive Adaptation Network for Arteriovenous Nicking Classification Using Synthetic Data".</li> 
				<li>[10/2023] Happy to receive the <hl_color>Best Paper Award</hl_color>  of  Distributed, Collaborative and Federated Learning (<strong>DeCAF</strong>) Workshop in MICCAI 2023:  "Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification". </li>
				<li>[10/2023] One paper accepted by <strong>Nature Communications</strong>: "Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification".</li>
				<li>[09/2023] One paper accepted by <strong>NeurIPS 2023</strong>: "Fairness-guided Few-shot Prompting for Large Language Models".</li> 
				<li>[09/2023] One paper accepted by <strong>IEEE TMI</strong>: "MG-Trans: Multi-scale Graph Transformer with Information Bottleneck for Whole Slide Image Classification".</li> 
				<li>[08/2023] One paper accepted by <strong>MedIA</strong>: "A Generic Fundus Image Enhancement Network Boosted by Frequency Self-supervised Representation Learning".</li>
				<li>[08/2023] One GAMMA Challenge summary paper accepted by <strong>MedIA</strong>: "GAMMA Challenge: Glaucoma grAding from Multi-Modality imAges".</li>

				
				<li>[06/2023] One papers accepted by <strong>ICCV 2023</strong>.</li>
				<li>[06/2023] 16 papers accepted by <strong>MICCAI 2023</strong>.</li>
				<li>[04/2023] Three papers accepted by <strong>ICML 2023</strong>.</li>
				<li>[04/2023] One paper accepted by <strong>Nature Machine Intelligence</strong>: "Federated Benchmarking of Medical Artificial Intelligence with MedPerf".</li>
				<li>[03/2023] One survey paper accepted by <strong>MedIA</strong>: "Transformers in Medical Imaging: A Survey".</li> 
				<li>[03/2023] One paper accepted by <strong>IEEE TMI</strong>.</li>
				<li>[03/2023] One paper accepted by <strong>IEEE TPAMI</strong>: "GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector".</li>
				<li>[03/2023] Glad to organize <strong>OMIA-X</strong> workshop with <strong>STAGE</strong> challenge on <strong>MICCAI 2023</strong>. </li>
				<li>[03/2023] Glad to organize <strong>REMIA</strong> workshop with <strong>ATLAS</strong> challenge on <strong>MICCAI 2023</strong>. </li> 
				<li>[02/2023] Two papers accepted by <strong>CVPR</strong>.</li>
				<li>[02/2023] One paper accepted by <strong>IEEE TNNLS</strong>.</li>
				<li>[02/2023] One paper accepted by <strong>IEEE TMI</strong>.</li>
				<li>[01/2023] The paper accepted by <strong>Nature Communications</strong>: "Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST".</li>
				<li>[01/2023] Serve as an Area Chair of <a href="https://conferences.miccai.org/2023/en/" target="_blank"><strong>MICCAI 2023</strong></a>. </li>
				<li>[01/2023] Be invited to serve as Associate Editor of <strong>IEEE TAI</strong>. </li>
				
				
				<li>------ <em>Happy New Year!</em> --------</li> 
				
				<li>[11/2022] The paper,  "Dual Multi-scale Mean Teacher Network for Semi-supervised Infection Segmentation in Chest CT Volume for COVID-19",  accepted by <strong>IEEE TCyb</strong>.</li> 
				
				<li>[10/2022] Happy to be <hl_color>'Top 2% Scientists Worldwide'</hl_color>, identified by Stanford University, 2022. <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw" target="_blank">[link]</a></li>
				<li>[10/2022] The paper,  "Contrastive Domain Adaptation with Consistency Match for Automated Pneumonia Diagnosis", accepted by <strong>MedIA</strong>.</li>
				<li>[09/2022] Happy to receive the <hl_color>Best Paper Award</hl_color>  of Ophthalmic Medical Image Analysis Workshop in MICCAI, 2022. </li>
				<li>[09/2022] Happy to receive the <hl_color>Best Paper Runner-up Award</hl_color>  of Resource-Efficient Medical Image Analysis Workshop in MICCAI, 2022. </li>
				<li>[08/2022] The paper, "Specificity-Preserving Federated Learning for MR Image Reconstruction", accepted by <strong>IEEE TMI</strong>. </li>
				<li>[07/2022] Happy to be granted as <strong>PI</strong> by <hl_grant>A*STAR Career Development Fund (CDF)</hl_grant>.</li>
				<li>[07/2022] One paper accepted by <strong>ECCV 2022</strong>.</li>
			
				<li>[06/2022] The paper, "Autoencoder in Autoencoder Networks", accepted by <strong>IEEE TNNLS</strong>.</li>
				<li>[06/2022] One paper accepted by <strong>ACM MM 2022</strong>.</li>
				<li>[06/2022] Be invited to serve as Associate Editor of <strong>IEEE TNNLS</strong>. </li>
				<li>[06/2022] Nine papers accepted by  <strong>MICCAI 2022</strong>. </li>

				<li>[05/2022] One paper for "Multi-Modal MR Reconstruction" accepted by <strong>IEEE TMI</strong>. </li>
				<li>[05/2022] One paper for "Lesion Segmentation in Fundus"  accepted by <strong>IEEE TMI</strong>. </li>
				<li>[05/2022] One paper for "Cataract classification in AS-OCT image" paper accepted by <strong>MedIA</strong>. </li>
				<li>[05/2022] Serve as Technical Program Committee member of  <a href="https://bhi-bsn-2022.org/" target="_blank"><strong>IEEE BHI 2022</strong></a>. </li>
				<li>[04/2022] One paper for "ADAM Challenge"  accepted by <strong>IEEE TMI</strong>. </li>
				<li>[04/2022] One paper for "Trustworthy Multi-view Classification"  accepted by  <strong>IEEE TPAMI</strong>. </li>
				<li>[03/2022] Serve as an Area Chair of  <a href="https://2022.ieeeicip.org/" target="_blank"><strong>IEEE ICIP 2022</strong></a>. </li>
				<li>[03/2022] Four papers accepted by  <strong>CVPR 2022</strong>.</li>
				<li>[02/2022] Happy by organize two workshops on <strong>MICCAI 2022</strong>: OMIA9 and REMIA. </li>
				<li>[02/2022] Happy to be granted as Co-PI by <hl_grant> AISG Tech Challenge Funding</hl_grant>.</li> 
				<li>[01/2022] One paper for <em>"Cataractous Fundus Restoration"</em> accepted by  <strong>IEEE TMI</strong>. </li> 
				<li>[01/2022] One paper for <em>"Motion Segmentation"</em> accepted by  <strong>IEEE TPAMI</strong>. </li>
				<li>[01/2022] Serve as an Area Chair of <a href="https://conferences.miccai.org/2022/en/" target="_blank"><strong>MICCAI 2022</strong></a>. </li>

				<li>------ <em>Happy New Year!</em> --------</li> 
				
				<li>[12/2021] One paper for <em>"RGB-D saliency detection"</em> accepted by <strong>IEEE TIP</strong>.
				</li>
				<li>[11/2021] Serve as an Area Chair of <a href="https://2022.midl.io/" target="_blank"><strong>MIDL 2022</strong></a>. 
				</li> 

				<li>[11/2021] Happy to be a member of the IEEE Bio Imaging and Signal Processing Technical Committee (<a href="https://signalprocessingsociety.org/community-involvement/bio-imaging-and-signal-processing/bisp-tc-home" target="_blank"><strong>BISP TC</strong></a>). 
				</li>
				
				<li>[09/2021] One paper for <em>"Anomaly Detection in Medical Image"</em> accepted by <strong>IEEE TMI</strong>.
				</li>
				<li>[09/2021] One paper for <em>"Trustworthy Multimodal Regression"</em> accepted by <strong>NeurIPS 2021</strong>.
				</li>
				<li>[09/2021] One paper for <em>"Angle-closure Assessment in AS-OCT"</em> accepted by <strong>IEEE TMI</strong>.
				</li>
				<li>[08/2021] One paper for <em>"Subspace Clustering"</em> accepted by <strong>IEEE TNNLS</strong>.
				</li>
				<li>[07/2021] One paper for <em>"Medical Image Enhancement"</em> accepted by <strong>IEEE TMI</strong>.
				</li>
				<li>[07/2021] <strong>4 papers</strong> accepted by <strong>ICCV 2021</strong>.</li>
				<li>[07/2021] Happy to receive the <hl_color><strong>Best Paper Award</strong></hl_color>  of <strong>IEEE ICME 2021</strong>! [<a href="https://2021.ieeeicme.org/best_paper_awards" target="_blank">Link</a>] </li>
				<li>[07/2021] One paper for <em>"Image Dehazing"</em> accepted by <strong>ACM MM 2021</strong>. 
				<li>[06/2021] <strong>5 papers</strong> accepted by  <strong>MICCAI 2021</strong>.  
				<li>[06/2021] One paper for <em>"MR Image Reconstruction"</em> accepted by <strong>IEEE TNNLS</strong>.
				</li>
				<li>[03/2021] <strong>2 papers</strong> (<em>"Co-saliency Detection"</em> and <em>"Video shadow
						Detection"</em>) accepted by  <strong>CVPR 2021</strong>.
				</li>
				<li>[02/2021] One paper for <em>"Co-saliency Detection"</em> accepted by  <strong>IEEE TPAMI</strong>.
				</li>
				<li>[01/2021] One paper for <em>"Breast Lesion Segmentation"</em> accepted by  <strong>MedIA</strong>.
				</li>
				<li>[01/2021] One paper for <em>"Fundus Image Analysis Survey"</em> accepted by <strong>MedIA</strong>.
				</li>
				<li>[01/2021] One paper for <em>"Uncertainty Estimation in Multi-view Learning"</em> accepted by  <strong>ICLR 2021</strong>.
				</li>
				<li>[01/2021] One paper for <em>"Saliency Detection Survey"</em>  accepted by <strong>IEEE TPAMI</strong>.
				</li> -->

			</ul>

		<!-- </div> -->

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />


			<h3>
				<a id="Services-page" class="anchor" href="#Services-page" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
						<img src="sub_img/activity-logo.gif" style="height:42px;"> Professional Activities:
			</h3>
			<ul>
				<!-- <li><strong>Memberships:</strong> -->
					<ul>
						<!-- <li> IEEE Senior Member. </li> -->
						<li> 
							<strong>Co-Chair</strong> of “Students & Young Professionals” Subcommittee in the IEEE Signal Processing Society (SPS) Technical Committee on Bio Imaging and Signal Processing (<a href="https://signalprocessingsociety.org/community-involvement/bio-imaging-and-signal-processing/bisp-tc-home" target="_blank"><strong>BISP</strong></a>).
						</li>
						<li>
							<strong>Member</strong> of the IEEE Engineering in Medicine and Biology Society (EMBS) Technical Committee on Biomedical Imaging and Image Processing (<a href="https://www.embs.org/biip/" target="_blank"><strong>BIIP</strong></a>).

						</li>
					</ul> 
				</li>

				<li><strong>Associate Editor:</strong>
					<ul>
						<li> IEEE Transactions on Medical Imaging (<strong>IEEE TMI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Neural Networks and Learning Systems (<strong>IEEE TNNLS</strong>), 2022 - present. </li>
						<li> IEEE Journal of Biomedical and Health Informatics (<strong>IEEE JBHI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Artificial Intelligence (<strong>IEEE TAI</strong>), 2023 - present. </li>
						<li> Pattern Recognition (<strong>PR</strong>), 2024 - present. </li>
						<!-- <li> Scientific Reports, 2021 - present. </li> -->
						<li> Visual Intelligence, 2024 - present. </li>
						<li> Meta-Radiology, 2023 - present. </li>
						<!-- <li> IEEE Access, 2018 - present. </li> -->
					</ul>
				</li> 

				<li><strong>Area Chair/Senior-PC:</strong>
					<ul>
						<li>
							MICCAI (2021-2023, 2025), IJCAI (2021), ACM MM (2024), AAAI (2022), MIDL (2022-2023), ICIP (2022-2024), BHI (2022), ICASSP (2023-2025), ISBI (2023-2025), ICLR (2025), EMBC (2025).
						</li>
					</ul>
				</li>


				<li><strong>Challenge Organizer:</strong>
					<ul>
						<li>
							"<strong>STAGE2</strong>: 2nd Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2024.  [<a href="https://aistudio.baidu.com/competition/detail/1167/0/introduction" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>ATLAS</strong>: A Tumor and Liver Automatic Segmentation" with the MICCAI 2023. [<a href="https://atlas-challenge.u-bourgogne.fr" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>STAGE</strong>: Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2023. [<a href="https://aistudio.baidu.com/aistudio/competition/detail/968/0/introduction" target="_blank">Link</a>]
						</li>

						<li>
							"<strong>GOALS</strong>: Glaucoma Oct Analysis and Layer Segmentation" with the MICCAI 2022.
							[<a href="https://aistudio.baidu.com/aistudio/competition/detail/230/0/introduction" target="_blank">Link</a>] 
						</li>
						<li>
							"<strong>GAMMA</strong>: Glaucoma Grading from Multi-Modality Images Challenge" with the MICCAI 2021.
							[<a href="https://gamma.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2202.06511" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>REFUGE2</strong>: 2nd Retinal Fundus Glaucoma Challenge" with the MICCAI 2020.
							[<a href="https://refuge.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.08994" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>ADAM</strong>: Automatic Detection challenge on Age-related Macular degeneration" with the ISBI 2020.
							[<a href="https://amd.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.07983" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>AGE</strong>: Angle closure Glaucoma Evaluation Challenge" with the MICCAI 2019.
							[<a href="https://age.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2005.02258" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>PALM</strong>: PathologicAL Myopia detection from retinal images" with the ISBI 2019. [<a href="https://palm.grand-challenge.org" target="_blank">Link</a>] 
							[<a href="https://doi.org/10.1038/s41597-024-02911-2" target="_blank">Data Summary</a>]
						</li>
						<li>
							"<strong>REFUGE</strong>: Retinal Fundus Glaucoma Challenge" with the MICCAI 2018. 
							[<a href="https://refuge.grand-challenge.org/" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/1910.03667" target="_blank">Summary Paper</a>]
						</li>
					</ul>
				</li>
			</ul>
			</p>

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />

			

			<h3>
				<a id="Highlighted-Paper-pages" class="anchor" href="#Highlighted-Paper-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
				<img src="sub_img/paper-logo.gif" style="height:42px;"> Highlighted Publications: 
			</h3>

			<strong>More publication in</strong>
				<a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<strong><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</strong>]</a>  

			<ul>

				<li><a href="https://www.nature.com/articles/s41467-025-60577-9" target="_blank"><strong>"Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model"</strong></a>,   <br>
					M. Wang, T. Lin, A. Lin, K. Yu, ......, C.-Y. Cheng, H. Chen, and <strong>H. Fu</strong>,<br>
					
					<i><strong>Nature Communications</strong>, 2025.</i> 
					<a href="https://github.com/LooKing9218/RetiZero" target="_blank">[Code]</a>
				  </li>

				  <li><strong>"An eyecare foundation model for clinical assistance: a randomized controlled trial"</strong>,   <br>
					Y. Wu, B. Qian, T. Li, Y. Qin, Z. Guan, T. Chen, Y. Jia, P. Zhang, D. Zeng,......, <strong>H. Fu</strong>, ......, Y. X. Wang, Y.-C. Tham, C.-Y. Cheng, T. Y. Wong, and B. Sheng, <br>  
					<i><strong>Nature Medicine</strong>, 2025.</i> 
					<a href="https://github.com/eyefm/EyeFM" target="_blank">[Code]</a>
				  </li>
				 


				<li> <a href="https://doi.org/10.1016/j.xcrm.2024.101876" target="_blank"><strong>"Enhancing Al Reliability: A Foundation Model with Uncertainty Estimation for Optical Coherence Tomography based Retinal Diseases Diagnosis"</strong></a>, <br>
					Y. Peng, A. Lin, M. Wang, T. Lin, ......, C.-Y. Cheng, <strong>H. Fu</strong>, and H. Chen, <br>  
					<i><strong>Cell Reports Medicine</strong>, 2024.</i> <a href="https://github.com/yuanyuanpeng0129/FMUE" target="_blank">[Code]</a>
					</li> 

					<li><a href="https://www.nature.com/articles/s41467-023-42444-7" target="_blank"><strong>"Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification"</strong></a>,   <br>
					M. Wang, T. Lin, L. Wang, A. Lin, ......, X. Chen, H. Chen, and <strong>H. Fu</strong>,<br>
					
					<i><strong>Nature Communications</strong>, 2023.</i> 
					<a href="https://github.com/LooKing9218/UIOS" target="_blank">[Code]</a>
				  </li>



			  <li><a href="https://www.nature.com/articles/s42256-023-00652-2" target="_blank"><strong>"Federated Benchmarking of Medical Artificial Intelligence with MedPerf"</strong></a>, <br>
				A. Karargyris, R. Umeton, M. J. Sheller, ......,  <strong>H. Fu</strong>, ......, J. M. Johnson, S. Bakas, and P. Mattson,<br>
			  <i><strong>Nature Machine Intelligence</strong>, 2023.</i> 
			  <a href="https://github.com/mlcommons/medperf" target="_blank">[Code]</a> 
			</li> 

			  <li><a href="https://www.nature.com/articles/s41467-023-36796-3" target="_blank"><strong>"Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST"</strong></a>,<br>
				Y. Long, K. S. Ang, M. Li, ......, <strong>H. Fu</strong>, ......, L. Liu, and J. Chen,<br>
			<i><strong>Nature Communications</strong>, 2023.</i> 
			<a href="https://github.com/JinmiaoChenLab/DeepST" target="_blank">[Code]</a> 
		  </li>


		  <li><a href="https://doi.org/10.1186/s13073-024-01283-x" target="_blank"><strong>"Unsupervised spatially embedded deep representation of spatial transcriptomics"</strong></a>,<br>  
					 H. Xu, <strong>H. Fu</strong>, Y. Long, K. S. Ang, R. Sethi, K. Chong, M. Li, R. Uddamvathanak, H. K. Lee, J. Ling, A. Chen, L. Shao, L. Liu and J. Chen,<br>  
					<i><strong>Genome Medicine</strong>, 2024.</i> 
					<a href="https://github.com/JinmiaoChenLab/SEDR/" target="_blank">[Code]</a>
					<a href="https://github.com/JinmiaoChenLab/SEDR_analyses/" target="_blank">[Analysis Code]</a> 
				  </li> 

				  <li> <a href="https://www.nature.com/articles/s41746-024-01292-5" target="_blank"><strong>"Early Detection of Dementia through Retinal Imaging and Trustworthy AI"</strong></a>, <br>
					J. Hao, W. Kwapong, T. Shen, <strong>H. Fu</strong>, ......,  S. Zhang, H. Qi, and Y. Zhao, <br>
					<i><strong>npj Digital Medicine</strong>, 2024.</i> 
					<a href="https://github.com/iMED-Lab/Eye-AD" target="_blank">[Code]</a> 
				  </li>   

				  <li> <a href="https://arxiv.org/abs/2404.06798" target="_blank"><strong>"Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding"</strong></a>, <br>
					K. Zou, Y. Bai, B. Liu, Y. Chen, Z. Chen, Y. Zhou, X. Yuan, M. Wang, X. Shen, X. Cao, Y. C. Tham, and, <strong>H. Fu</strong>, <br>  
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2025.</i> 
					<a href="https://github.com/Cocofeat/uMedGround" target="_blank">[Code]</a>
					</li> 

					<li> <a href="https://arxiv.org/abs/2508.03197" target="_blank"><strong>"Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network"</strong></a>, <br>
					T. Chen, D. Zhang, D. Chen, <strong>H. Fu</strong>, K. Jin, S. Wang, L. D. Cohen, Y. Zhao, Q. Yi, and J. Zhang, <br>  
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2025.</i>  
					<a href="https://github.com/jiongzhang-john/CNVSeg-Dataset" target="_blank">[Code]</a>
					</li> 


				<li> <a href="https://arxiv.org/abs/2408.13161" target="_blank"><strong>"Say No to Freeloader: Protecting Intellectual Property of Your Deep Model"</strong></a>, <br>
					L. Wang, M. Wang, <strong>H. Fu</strong>, and D. Zhang, <br>  
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2024.</i> <a href="https://github.com/LyWang12/CUPI-Domain" target="_blank">[Code]</a>
					</li> 

				<li> <a href="https://arxiv.org/abs/2205.12857" target="_blank"><strong>"Structure Unbiased Adversarial Model for Medical Image Translation"</strong></a>,<br>
					T. Zhang, S. Zheng, J. Cheng, X. Jia, J. Bartlett, X. Cheng, Z. Qiu, <strong>H. Fu</strong>, J. Liu, A. Leonardis, and J. Duan,<br>  
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2024.</i> <a href="https://traceable-translation.github.io/" target="_blank">[Online Demo]</a>
				  </li> 

 
		  <li><a href="https://arxiv.org/abs/2205.15469" target="_blank"><strong>"GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector"</strong></a>,<br>
			P. Zheng, <strong>H. Fu</strong>, D.-P. Fan, Q. Fan, J. Qin, Y.-W. Tai, C.-K. Tang, and L. V. Gool,<br>  
			<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2023.</i>
			<a href="https://github.com/ZhengPeng7/GCoNet_plus" target="_blank">[Code]</a>
		  </li> 
		   

			  <li><a href="https://arxiv.org/abs/2204.11423" target="_blank"><strong>"Trusted Multi-View Classification with Dynamic Evidential Fusion"</strong></a>,
				<br>
				Z. Han, C. Zhang, <strong>H. Fu</strong>, and J. T. Zhou,<br>
				<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2023.</i> 
				<a href="https://github.com/hanmenghan/TMC" target="_blank">[Code]</a>
 
			  </li>
			  
				<li><a href="https://arxiv.org/abs/2202.04861" target="_blank"><strong>"Consistency and Diversity induced Human Motion Segmentation"</strong></a>,<br>
					T. Zhou, <strong>H. Fu</strong>, C. Gong, L. Shao, F. Porikli, H. Ling, and J. Shen,</em><br>
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2023.</i>  
				  </li> 
			
				  
				<li><a href="https://arxiv.org/abs/2007.03380" target="_blank"><strong>"Re-thinking Co-Salient Object Detection"</strong></a>, <br>
					D.-P. Fan, T. Li, Z. Lin, G.-P. Ji, D. Zhang, M.-M. Cheng, <strong>H. Fu</strong>, and J. Shen, <br>
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2022.</i> 
					<a href="https://dengpingfan.github.io/papers/[2021][TPAMI]CoSOD3k_Chinese.pdf" target="_blank">[Chinese
						version]</a> 
					<a href="https://github.com/DengPingFan/CoEGNet" target="_blank">[Code]</a>
 
				</li>

				<li><a href="https://arxiv.org/abs/1904.09146" target="_blank"><strong>"Salient Object Detection in the Deep Learning Era: An In-Depth Survey",</strong></a><br>
					W. Wang, Q. Lai, <strong>H. Fu</strong>, J. Shen, H. Ling, and R. Yang,<br>
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2022.</i>
					<a href="https://github.com/wenguanwang/SODsurvey" target="_blank">[Project]</a>  
 
				</li>

				<li><a href="https://arxiv.org/abs/2011.06170" target="_blank"><strong>"Deep Partial Multi-View Learning"</strong></a>,<br>
					C. Zhang, Y. Cui, Z. Han, J. T. Zhou, <strong>H. Fu</strong>, and Q. Hu,<br>
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2022.</i>
					
					<a href="https://github.com/hanmenghan/CPM_Nets" target="_blank">[Code]</a>  
		 
				</li>    
 
				<li><a href="https://www.researchgate.net/publication/328479701_Generalized_Latent_Multi-View_Subspace_Clustering" target="_blank"><strong>"Generalized Latent Multi-view Subspace Clustering"</strong></a>,<br>
					C. Zhang, <strong>H. Fu</strong>, Q. Hu, X. Cao, Y. Xie, D. Tao, and D. Xu, <br> 
					<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>), 2020.</i> 
					<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/LMSC_CVPR2017_Zhang.rar">[Code]</a> 
					<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/ORL_mtv.rar">[Data]</a>
					 
					 
				</li>    
			</ul> 

			 

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />

			<h3>
				<a id="awards-pages" class="anchor" href="#awards-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
				<img src="sub_img/award-logo.gif" style="height:42px;"> Recognitions & Awards: 
			</h3>
			<ul> 


				<li> 
					2020-2024: <strong>World's Top 2% Scientists List</strong>, by Stanford/Elsevier (in category Artificial Intelligence & Image Processing).
					<a href="https://topresearcherslist.com" target="_blank">[Link]</a>
				</li>

				<li> 
					2024: <hl_color>Highly Cited Researcher</hl_color>, by Web of Science (in the field of Cross-Field), Clarivate. 
				</li>

				<li>2024: <strong>First Place</strong> in <a href="https://2024.asiateleophth.org/big-data-competition/" target="_blank">"Big Data Competition"</a> in APTOS-APOIS 2024.</li>

				<li>
					2024: <strong>Best Paper Award</strong> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI. 
				</li>

				<li>
					2024: Finalist of the Young Scientist Publication Impact Award in MICCAI.
				</li>

				

				<li>
					2024: <strong>Collaborative Paper Award</strong> by Biomedical Research Council (BMRC), A*STAR.
				</li>
				<li>
					2024: <strong>Outstanding Presentation</strong> under the Computing, Data and Digital Sciences track at A*STAR CDF Day, 2024.
				</li>

				<li>
					2023: <strong>Honorable Mention Award</strong> by Computational Visual Media Journal. 
					<a href="https://link.springer.com/journal/41095/updates/26965690" target="_blank">[Link]</a>
				</li> 
				
				<li>
					2023: <strong>Achievement Award</strong>  of  2023 TOP 100 Benchmarks & Evaluation by International Open Benchmark Council.
				</li>
				<li>
					2023: <strong>Best Paper Award</strong> of  Distributed, Collaborative and Federated Learning (DeCAF) Workshop in MICCAI. 
				</li>
				
				<li>
					2022: <strong>Best Paper Award</strong> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI. 
				</li>
				<li>
					2022: <strong>Best Paper Runner-up Award</strong> of Resource-Efficient Medical Image Analysis (REMIA) Workshop in MICCAI. 
				</li>
				
				<li>
					2021: <strong>Best Paper Award</strong> in IEEE International Conference on Multimedia & Expo (ICME). 
					<a href="http://2021.ieeeicme.org/2021.ieeeicme.org/best_paper_awards.html" target="_blank">[Link]</a>
				</li> 
				
				<li>
					2021: Finalist of the Young Scientist Publication Impact Award in MICCAI.
				</li>
				<li>
					2021: <strong>Most Influential Paper (Application) Award</strong> in Jittor Developer Conference.
				</li>
				<!-- <li>
					2020-2022: IEEE TMI Distinguished Reviewer Gold Level.
				</li>
				<li>
					2021: Outstanding Reviewer in CVPR.
				</li> -->
				<li>
					2014 China Computer Federation (CCF) Outstanding Dissertation Nomination.
					<a href="https://www.ccf.org.cn/c/2014-12-14/647609.shtml" target="_blank">[Link]</a>
				</li>

			</ul>
			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div> 
			
			  <br>

		</section>

	</div>
</body>

</html>
