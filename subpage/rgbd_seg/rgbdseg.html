<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>RGBD Segmentation</title>

    <link rel="stylesheet" href="../../stylesheets/styles.css">
    <link rel="stylesheet" href="../../stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Huazhu Fu</h1>
<p>
<small>huazhufu (AT) gmail (DOT) com </small><br><br>
<a href="https://github.com/HzFu" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/f/Fu:Huazhu" target="_blank">[DBLP]</a>  <br>
<a href="http://scholar.google.com.sg/citations?user=jCvUBYMAAAAJ" target="_blank">[Google Scholar]</a> </p> <br>
<p class="view"><a href="../../index.html">Homepage</a></p>
<p class="view"><a href="../publication.html">Publications</a></p>
<p class="view"><a href="../codes.html">Codes and Data</a></p>
<p class="view"><a href="../projects.html">Projects</a></p>
      </header>

      <section>

<h2>
<a id="project_title" class="anchor" href="#project_title" aria-hidden="true"><span class="octicon octicon-link"></span></a>Object-based RGBD co-segmentation</h2>




<h4>
<a id="Introduction-page" class="anchor" href="#Introduction-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction:</h4>

<p>We present an object-based co-segmentation method that takes advantage of depth data and is able to correctly handle noisy images in which the common foreground object is missing. With RGBD images, our method utilizes the depth channel to enhance identification of similar foreground objects via a proposed RGBD co-saliency map, as well as to improve detection of object-like regions and provide depth- based local features for region comparison. To accurately deal with noisy images where the common object appears more than or less than once, we formulate co-segmentation in a fully-connected graph structure together with mutual exclusion (mutex) constraints that prevent improper solutions. Experiments show that this object-based RGBD co-segmentation with mutex constraints outperforms related techniques on an RGBD co-segmentation dataset, while effectively processing noisy images. Moreover, we show that this method also provides performance comparable to state- of-the-art RGB co-segmentation techniques on regular RGB images with depth maps estimated from them.</p>

<div style="text-align: center; display: block; margin-right: auto;">
<img src="framework.jpg" border="0" width="600"><br></div><br>


<hr />
<h4>Paper:</h4>
    
<p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Fu_Object-Based_RGBD_Image_2015_CVPR_paper.html" target="_blank"><strong>"Object-based RGBD Image Co-segmentation with Mutex Constraint"</strong></a><br>
Huazhu Fu, Dong Xu, Stephen Lin, Jiang Liu,<br>
in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015, pp. 4428-4436. <br>
</p>

<hr />
<h4>Dataset:</h4>
<p> We biuld a <a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!269&authkey=!AHUBN0lk5kQLWzQ&ithint=file%2czip" target="_blank"><font color="#ff0000">RGBD image co-segmentation dataset</font></a> (~102MB), which contains 16 image sets, each of 6 to 17 images taken from indoor scenes with one common foreground object (193 images in total).</p>

<h4> Result: </h4>
<p> The result of our method can be downloaded from: 
[<a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!255&authkey=!ANuwbADPp9Dsm4Q&ithint=file%2czip"><font color="#ff0000">Here</font></a>]  (~20MB), which contains the RGBD co-saliency map and co-segmentation mask.
</p>

<hr />
<h4>Related Works:</h4>

<p>[1] <a href="http://dx.doi.org/10.1109/TIP.2015.2442915" target="_blank"><strong>"Object-based Multiple Foreground Video Co-segmentation via Multi-state Selection Graph"</strong></a><br> 
Huazhu Fu, Dong Xu, Bao Zhang, Stephen Lin, Rabab K. Ward,<br>
<em>IEEE Transactions on Image Processing (TIP)</em>, vol. 24, no. 11, pp. 3415-3424, 2015. <br>
[<a href="https://github.com/HzFu/VideoCoSeg_MSG" target="_blank"><font color="#ff0000">Code</font></a>] [<a href="../video_coseg/video_coseg.html" target="_blank"><font color="#ff0000">Project</font></a>] </p>

<p>[2] <a href="http://dx.doi.org/10.1109%2FTIP.2013.2260166" target="_blank"><strong>"Cluster-based Co-saliency Detection"</strong></a><br>
Huazhu Fu, Xiaochun Cao, Zhuowen Tu,<br>
<em>IEEE Transactions on Image Processing (TIP)</em>, vol. 22, no. 10, pp. 3766-3778, 2013. <br>
[<a href="https://github.com/HzFu/Cosaliency_tip2013" target="_blank"><font color="#ff0000">Code</font></a>] </p>

<p>[3] <a href="http://dx.doi.org/10.1109/TIP.2014.2332399" target="_blank"><strong>"Self-adaptively Weighted Co-saliency Detection via Rank Constraint"</strong></a><br>
Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, Wei Feng,<br>
<em>IEEE Transactions on Image Processing (TIP)</em>, vol. 23, no. 9, pp. 4175-4186, 2014. <br>
[<a href="https://github.com/HzFu/SACS_TIP2014" target="_blank"><font color="#ff0000">Code</font></a>]</p>

      </section>
      <footer>
        <p>This page is maintained by <a href="http://hzfu.github.io/">Huazhu Fu</a></p>
      </footer>
    </div>
    <script src="../../javascripts/scale.fix.js"></script>
  </body>
</html>
