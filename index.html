<!doctype html>
 
<html class="no-js" lang="en" >  
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="chrome=1"> 
	<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
	<link rel="icon" type="image/x-icon" href="favicon.ico">
	<link rel="Bookmark" type="image/x-icon" href="favicon.ico">
	<link rel="apple-touch-icon" type="image/x-icon" href="favicon.png">
	<link rel="stylesheet" href="css/styles.css">  
	
	<title>Homepage of Huazhu FU</title>
	
</head>

<body> 
	<div id="wrapper"> 
		<section>
			<table border="0" id="table1" width="100%">
				<tbody>
					<tr> 
						<td >
							<p align="left">
								<img border="0" src="sub_img/hz_photo.jpg" width="250">
							</p>

						</td> 

						<td>
							<h1> <a id="top-page" class="anchor" href="#top-page" aria-hidden="true"><span
								class="octicon octicon-link"></span></a>
								Huazhu FU</h1>
							<p>
								<b>Principal Scientist</b><br>
								<em> Institute of High Performance Computing (IHPC)<br>
								Agency for Science, Technology and Research (A*STAR), Singapore. </em>
							</p>
							<p>
								<strong>Email:</strong>  
								<em>hzfu(AT)ieee(DOT)org</em> 
							</p> 

							
						</td>  
						
						
					</tr>
				</tbody>
			</table>

			<b>
				<a href="#Services-page">[<ud>Professional Activities</ud>]</a> 
				- 
				<a href="#Highlighted-Paper-pages">[<ud>Highlighted Publications</ud>]</a> 
				- 
				<a href="#awards-pages">[<ud>Recognitions & Awards</ud>]</a>   
				- 
				<a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<b><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</b>]</a> 
				
			</b>
			<hr /> 

			<strong>My research focuses on:</strong>  
			<ul>
				<li> <strong>AI for Healthcare:</strong>
					Medical Image Analysis, ⭐ Medical Vision-Language Model, ⭐ Medical Foundation Model. 
				</li>
				<li> <strong>Trustworthy AI:</strong> 
					Federated Learning, ⭐ Uncertainty Estimation,  ⭐ Visual Grounding. 
				</li> 
			</ul>
				

			<hr />

			<!-- Section: Open position 

				<table border="0" id="table1" width="100%">
				<tbody>
			<tr> 
				<td style="width:15%">
					<p align="left">
						<img border="0" src="sub_img/open-position-logo.png" width="120">
					</p>
				</td>
			
				<td > 
					 <li>
						<b><hl_color>Scientist Position:</hl_color></b>
						We have several <a href="./job_poster.html" target="_blank"><b>Research Scientist Positions in AI for Healthcare and Trustworthy AI</b></a> available at IHPC, A*STAR, Singapore.  			  
					<li>
						<b><hl_color>PhD Scholarships:</hl_color></b>
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa" target="_blank"><b>Singapore International Graduate Award (SINGA) Scholarship</b></a> supports international students who wish to pursue their PhDs, collaborated with Singapore Universities (NUS, NTU, and SUTD). 
					</li>  
					<li>
						<b><hl_color>Visiting PhD Student:</hl_color></b> 
						<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-research-attachment-programme" target="_blank"><b>A*STAR Research Attachment Programme (ARAP)</b></a> supports PhD students from overseas universities to spend a minimum of one to a maximum of two years at A*STAR Research Institutes under the joint supervision.
					</li> 
					<li>
						<b><hl_color>Chinese PhD Student:</hl_color></b> 
						We are also looking for visiting PhD students from China to spend a minimum of one year at IHPC, under <a href="https://www.csc.edu.cn/chuguo" target="_blank"><b>Chinese CSC Scholarship</b></a>. 
						<em><hl_color>Requirements:</hl_color> Due to the limited headcount, we have to narrow down the list as: <strong>1)</strong> Having at least <strong>TWO</strong> first-author publications at top-tier conferences (e.g., CVPR, ICCV, ICLR) or journals (e.g., TPAMI, IJCV, TMI, TIP).
						<strong>2)</strong> Familiarity with one or more of the following will be viewed favorable: <strong>efficient learning</strong> (e.g., semi/self/weakly supervised learning, and domain adaptation), <strong>large foundation model</strong> (e.g., vision-language model, and generative model), and <strong>trustworthy AI</strong> (e.g., federated learning, uncertainty estimation, and explainable AI). </em>  
					</li> 
			
				</td> 
			</tr>
			</tbody>
			</table>
			<hr/>  
		--> 

			<h3>
				<a id="Services-page" class="anchor" href="#Services-page" aria-hidden="true"><span
						class="octicon octicon-link"></span></a>
						<img src="sub_img/activity-logo.gif" style="height:42px;"> Professional Activities:
			</h3>
			
			<ul> 
					<ul>
						<!-- <li> IEEE Senior Member. </li> -->
						<li> 
							<strong>Member</strong> of the IEEE Signal Processing Society (SPS) Technical Committee on Bio Imaging and Signal Processing (<a href="https://signalprocessingsociety.org/community-involvement/bio-imaging-and-signal-processing/bisp-tc-home" target="_blank"><strong>BISP</strong></a>).
						</li>
						<li>
							<strong>Member</strong> of the IEEE Engineering in Medicine and Biology Society (EMBS) Technical Committee on Biomedical Imaging and Image Processing (<a href="https://www.embs.org/biip/" target="_blank"><strong>BIIP</strong></a>).

						</li>
					</ul>  

				<li><strong>Associate Editor:</strong>
					<ul>
						<li> IEEE Transactions on Medical Imaging (<strong>IEEE TMI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Neural Networks and Learning Systems (<strong>IEEE TNNLS</strong>), 2022 - present. </li>
						<li> IEEE Journal of Biomedical and Health Informatics (<strong>IEEE JBHI</strong>), 2020 - present. </li>
						<li> IEEE Transactions on Artificial Intelligence (<strong>IEEE TAI</strong>), 2023 - present. </li>
						<li> Pattern Recognition (<strong>PR</strong>), 2024 - present. </li> 
						<!-- <li> Visual Intelligence, 2024 - present. </li> -->
						<!-- <li> Meta-Radiology, 2023 - present. </li> --> 
					</ul>
				</li>  

				<li><strong>Challenge Organizer:</strong>
					<ul>
						<li>
							"<strong>GAVE</strong>: Generalized Analysis of Vessels in Eye" with the MICCAI 2025.  
							[<a href="https://aistudio.baidu.com/competition/detail/1315/0/introduction" target="_blank">Link</a>] 
						</li>
						<li>
							"<strong>STAGE2</strong>: 2nd Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2024.  
							[<a href="https://aistudio.baidu.com/aistudio/competition/detail/1167" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>ATLAS</strong>: A Tumor and Liver Automatic Segmentation" with the MICCAI 2023. 
							[<a href="https://atlas-challenge.u-bourgogne.fr" target="_blank">Link</a>] 
						</li>

						<li>
							"<strong>STAGE</strong>: Structural-Functional Transition in Glaucoma Assessment" with the MICCAI 2023. 
							[<a href="https://aistudio.baidu.com/aistudio/competition/detail/1167" target="_blank">Link</a>]
						</li>

						<li>
							"<strong>GOALS</strong>: Glaucoma Oct Analysis and Layer Segmentation" with the MICCAI 2022.
							[<a href="https://aistudio.baidu.com/competition/detail/783/0/introduction" target="_blank">Link</a>] 
						</li>
						<li>
							"<strong>GAMMA</strong>: Glaucoma Grading from Multi-Modality Images Challenge" with the MICCAI 2021.
							[<a href="https://gamma.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2202.06511" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>REFUGE2</strong>: 2nd Retinal Fundus Glaucoma Challenge" with the MICCAI 2020.
							[<a href="https://refuge.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.08994" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>ADAM</strong>: Automatic Detection challenge on Age-related Macular degeneration" with the ISBI 2020.
							[<a href="https://amd.grand-challenge.org" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/2202.07983" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>AGE</strong>: Angle closure Glaucoma Evaluation Challenge" with the MICCAI 2019.
							[<a href="https://age.grand-challenge.org/" target="_blank">Link</a>] 
							[<a href="https://arxiv.org/abs/2005.02258" target="_blank">Summary Paper</a>]
						</li>
						<li>
							"<strong>PALM</strong>: PathologicAL Myopia detection from retinal images" with the ISBI 2019. [<a href="https://palm.grand-challenge.org" target="_blank">Link</a>] 
							[<a href="https://doi.org/10.1038/s41597-024-02911-2" target="_blank">Data Summary</a>]
						</li>
						<li>
							"<strong>REFUGE</strong>: Retinal Fundus Glaucoma Challenge" with the MICCAI 2018. 
							[<a href="https://refuge.grand-challenge.org/" target="_blank">Link</a>]
							[<a href="https://arxiv.org/abs/1910.03667" target="_blank">Summary Paper</a>]
						</li>
					</ul>
				</li>
			</ul> 

			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			  </div>
			<hr />
  
			<h3>
				<a id="Highlighted-Paper-pages" class="anchor" href="#Highlighted-Paper-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
				<img src="sub_img/paper-logo.gif" style="height:42px;"> Highlighted Publications: 
			</h3>

			<strong>Full publication list:</strong>
				<a href="https://scholar.google.com/citations?hl=en&user=jCvUBYMAAAAJ" target="_blank">[<strong><font color="#4285F4">G</font><font color="#DB4437">o</font><font color="#F4B400">o</font><font color="#4285F4">g</font><font color="#0F9D58">l</font><font color="#DB4437">e</font> Scholar</strong>]</a>  

			<table border="0" width="100%"> 
				<tr>
					<th style="width:10%"></th>
					<th></th> 
				</tr>
				

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/npj_Digital_Medicine_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41746-025-01988-2" target="_blank"><strong>"A deep learning based automatic report generator for retinal optical coherence tomography images"</strong></a>, <br>
					
							X. Chen, <strong>H. Fu</strong>, J. Wang, ......, C.P. Pang, and H. Chen, <br>
					
							<i><strong>npj Digital Medicine</strong>, 2025.</i> 
							<a href="https://github.com/Poizon1213/Retinal_OCT_Report_Automatic_Generation" target="_blank">[Code]</a>  
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-025-60577-9" target="_blank"><strong>"Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model"</strong></a>,   <br>
							M. Wang, T. Lin, A. Lin, K. Yu, ......, C.-Y. Cheng, H. Chen, and <strong>H. Fu</strong>,<br>  
							<i><strong>Nature Communications</strong>, 2025.</i>  
							<a href="https://github.com/LooKing9218/RetiZero" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Medicine_Logo.png" width="100"></p>
					</td>  
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41591-025-03900-7" target="_blank"><strong>"An Eyecare Foundation Model for Clinical Assistance: a Randomized Controlled Trial"</strong></a>,   <br>
							Y. Wu, B. Qian, T. Li, Y. Qin,......, <strong>H. Fu</strong>, ......, T. Y. Wong, and B. Sheng, <br>  
							<i><strong>Nature Medicine</strong>, 2025.</i> 
							<a href="https://github.com/eyefm/EyeFM" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr> 

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2404.06798" target="_blank"><strong>"Uncertainty-aware Medical Diagnostic Phrase Identification and Grounding"</strong></a>, <br>
							K. Zou, Y. Bai, B. Liu, Y. Chen, Z. Chen, Y. Zhou, X. Yuan, M. Wang, X. Shen, X. Cao, Y. C. Tham, and, <strong>H. Fu</strong>, <br>
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2025.</i> 
							<a href="https://github.com/Cocofeat/uMedGround" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2508.03197" target="_blank"><strong>"Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network"</strong></a>, <br>
							T. Chen, D. Zhang, D. Chen, <strong>H. Fu</strong>, K. Jin, S. Wang, L. D. Cohen, Y. Zhao, Q. Yi, and J. Zhang, <br>
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2025.</i>
							<a href="https://github.com/jiongzhang-john/CNVSeg-Dataset" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Cell-Reports-Medicine-Logo.jpeg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://doi.org/10.1016/j.xcrm.2024.101876" target="_blank"><strong>"Enhancing Al Reliability: A Foundation Model with Uncertainty Estimation for Optical Coherence Tomography based Retinal Diseases Diagnosis"</strong></a>, <br>
							Y. Peng, A. Lin, M. Wang, T. Lin, ......, C.-Y. Cheng, <strong>H. Fu</strong>, and H. Chen, <br> 
							<i><strong>Cell Reports Medicine</strong>, 2024.</i> <a href="https://github.com/yuanyuanpeng0129/FMUE" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2408.13161" target="_blank"><strong>"Say No to Freeloader: Protecting Intellectual Property of Your Deep Model"</strong></a>, <br>
							L. Wang, M. Wang, <strong>H. Fu</strong>, and D. Zhang, <br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2024.</i> <a href="https://github.com/LyWang12/CUPI-Domain" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://arxiv.org/abs/2205.12857" target="_blank"><strong>"Structure Unbiased Adversarial Model for Medical Image Translation"</strong></a>,<br>
					
							T. Zhang, S. Zheng, J. Cheng, X. Jia, J. Bartlett, X. Cheng, Z. Qiu, <strong>H. Fu</strong>, J. Liu, A. Leonardis, and J. Duan,<br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2024.</i> <a href="https://traceable-translation.github.io/" target="_blank">[Online Demo]</a>
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/npj_Digital_Medicine_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41746-024-01292-5" target="_blank"><strong>"Early Detection of Dementia through Retinal Imaging and Trustworthy AI"</strong></a>, <br>
					
							J. Hao, W. Kwapong, T. Shen, <strong>H. Fu</strong>, ......,  S. Zhang, H. Qi, and Y. Zhao, <br>
					
							<i><strong>npj Digital Medicine</strong>, 2024.</i> 
					
							<a href="https://github.com/iMED-Lab/Eye-AD" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-023-42444-7" target="_blank"><strong>"Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification"</strong></a>,   <br>
					
							M. Wang, T. Lin, L. Wang, A. Lin, ......, X. Chen, H. Chen, and <strong>H. Fu</strong>,<br>
					
							<i><strong>Nature Communications</strong>, 2023.</i> 
					
							<a href="https://github.com/LooKing9218/UIOS" target="_blank">[Code]</a>
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Machine_Intelligence_log.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s42256-023-00652-2" target="_blank"><strong>"Federated Benchmarking of Medical Artificial Intelligence with MedPerf"</strong></a>, <br>
					
							A. Karargyris, R. Umeton, M. J. Sheller, ......,  <strong>H. Fu</strong>, ......, J. M. Johnson, S. Bakas, and P. Mattson,<br>
					
							<i><strong>Nature Machine Intelligence</strong>, 2023.</i> 
					
							<a href="https://github.com/mlcommons/medperf" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr>

				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/Nature_Communications_Logo.jpg" width="100"></p>
					</td>   
					<td > 
						<p align="left">
							<a href="https://www.nature.com/articles/s41467-023-36796-3" target="_blank"><strong>"Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST"</strong></a>,<br>
					
							Y. Long, K. S. Ang, M. Li, ......, <strong>H. Fu</strong>, ......, L. Liu, and J. Chen,<br>
					
							<i><strong>Nature Communications</strong>, 2023.</i> 
					
							<a href="https://github.com/JinmiaoChenLab/DeepST" target="_blank">[Code]</a> 
						</p>

					</td> 
				</tr> 


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2205.15469" target="_blank"><strong>"GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector"</strong></a>,<br>
					
							P. Zheng, <strong>H. Fu</strong>, D.-P. Fan, Q. Fan, J. Qin, Y.-W. Tai, C.-K. Tang, and L. V. Gool,<br>  
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i>
					
							<a href="https://github.com/ZhengPeng7/GCoNet_plus" target="_blank">[Code]</a>

						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2204.11423" target="_blank"><strong>"Trusted Multi-View Classification with Dynamic Evidential Fusion"</strong></a>,<br>
					
							Z. Han, C. Zhang, <strong>H. Fu</strong>, and J. T. Zhou,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i> 
					
							<a href="https://github.com/hanmenghan/TMC" target="_blank">[Code]</a>
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2202.04861" target="_blank"><strong>"Consistency and Diversity induced Human Motion Segmentation"</strong></a>,<br>
					
							T. Zhou, <strong>H. Fu</strong>, C. Gong, L. Shao, F. Porikli, H. Ling, and J. Shen,</em><br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2023.</i>  
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2007.03380" target="_blank"><strong>"Re-thinking Co-Salient Object Detection"</strong></a>, <br>
					
							D.-P. Fan, T. Li, Z. Lin, G.-P. Ji, D. Zhang, M.-M. Cheng, <strong>H. Fu</strong>, and J. Shen, <br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>  
					
							<a href="https://github.com/DengPingFan/CoEGNet" target="_blank">[Code]</a> 
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/1904.09146" target="_blank"><strong>"Salient Object Detection in the Deep Learning Era: An In-Depth Survey",</strong></a><br>
					
							W. Wang, Q. Lai, <strong>H. Fu</strong>, J. Shen, H. Ling, and R. Yang,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>
					
							<a href="https://github.com/wenguanwang/SODsurvey" target="_blank">[Project]</a> 
					
						</p>

					</td> 
				</tr>



				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://arxiv.org/abs/2011.06170" target="_blank"><strong>"Deep Partial Multi-View Learning"</strong></a>,<br>
					
							C. Zhang, Y. Cui, Z. Han, J. T. Zhou, <strong>H. Fu</strong>, and Q. Hu,<br>
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2022.</i>
					
							<a href="https://github.com/hanmenghan/CPM_Nets" target="_blank">[Code]</a> 
					
						</p>

					</td> 
				</tr>


				<tr> 
					<td>
						<p align="left"><img border="0" src="sub_img/IEEE_TPAMI_logo.png" width="100"></p>
					</td>   
					<td > 
						<p align="left">

							<a href="https://www.researchgate.net/publication/328479701_Generalized_Latent_Multi-View_Subspace_Clustering" target="_blank"><strong>"Generalized Latent Multi-view Subspace Clustering"</strong></a>,<br>
					
							C. Zhang, <strong>H. Fu</strong>, Q. Hu, X. Cao, Y. Xie, D. Tao, and D. Xu, <br> 
					
							<i><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong>, 2020.</i> 
					
							<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/LMSC_CVPR2017_Zhang.rar">[Code]</a> 
					
							<a href="http://cic.tju.edu.cn/faculty/zhangchangqing/code/ORL_mtv.rar">[Data]</a>
				</li>    
					
						</p>

					</td> 
				</tr>

			</table>   



			<div align="right">
			<a href="#top-page">[<b>Back to top</b>]</a>
			</div>
			<hr />

			<h3>
			<a id="awards-pages" class="anchor" href="#awards-pages" aria-hidden="true" ><span class="octicon octicon-link"></span></a>
			<img src="sub_img/award-logo.gif" style="height:42px;"> Recognitions & Awards: 
			</h3>

			<ul> 

			<li> 
				Since 2024: <strong><hl_award>Highly Cited Researcher</hl_award>, by Clarivate</strong> (in the field of Cross-Field). 
				<a href="https://clarivate.com/highly-cited-researchers/" target="_blank">[Link]</a>
			</li>

			<li> 
				Since 2021: <strong><hl_color>World's Top 2% Scientists</hl_color>, by Stanford University</strong> (in category Artificial Intelligence & Image Processing).
				<a href="https://topresearcherslist.com/Home/Profile/985593" target="_blank">[Link]</a>
			</li>

			<li>
				2025: <strong><hl_award>Best Paper Award</hl_award> of IEEE Journal of Biomedical and Health Informatics (JBHI)</strong>,
				<i>for "Uncertainty-Inspired Multi-Task Learning in Arbitrary Scenarios of ECG Monitoring".</i>
				<a href="https://www.embs.org/jbhi/best-reviewers/best-papers-awards/" target="_blank">[Link]</a>  
			</li>

			<li>
				2025: <strong><hl_award>Science and Technology Progress Award (First Prize)</hl_award> by the China Medicine Education Association</strong> 
				(中国医药教育协会, <hl_award><strong>科学技术进步奖一等奖</strong></hl_award>), 
				<i>for "Key Technological Innovations and Applications in Retinal Imaging Diagnosis and Treatment".</i>
				<a href="https://cmea.org.cn/mainLayout/article/detail?id=3587&type=08ac71a3-6d5d-455b-9624-47b7256fea11&from=associationOverview%2Fnotice" target="_blank">[Chinese Link]</a> 
				<a href="./Awards/2025-ST-Progress-Award-CMEA.pdf" target="_blank">[Certification with google translation]</a>
				
			</li> 

			<li>
				2025: <strong><hl_award>Young Scientist Publication Impact Award</hl_award> in MICCAI</strong>, 
				<i>for "PraNet: Parallel Reverse Attention Network for Polyp Segmentation".</i>
				<a href="https://miccai.org/index.php/about-miccai/awards/young-scientist-impact-award/" target="_blank">[Link]</a>
				
			</li> 
 
			<li>
				2024: <hl_color>First Place</hl_color> in <a href="https://2024.asiateleophth.org/big-data-competition/" target="_blank">"Big Data Competition"</a> in APTOS-APOIS 2024.
			</li>

			<li>
				2024: <strong><hl_color>Best Paper Award</hl_color> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI</strong>, 
				<i>for "Enhancing Large Foundation Models to Identify Fundus Diseases Based on Contrastive Enhanced Low-Rank Adaptation Prompt".</i> 
				<a href="./Awards/2024-OMIA-best-oral.pdf" target="_blank">[Certification]</a>
			</li>
 
			<!-- <li>
				2024: <strong><hl_color>Collaborative Paper Award</hl_color> by Biomedical Research Council (BMRC), A*STAR</strong>, 
				<i>for "Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST".</i>
			</li>
				
			<li>
				2024: <strong><hl_color>Outstanding Presentation</hl_color> under the Computing, Data and Digital Sciences track at A*STAR CDF Day</strong>, 2024.
			</li> -->

			<li>
				2023: <strong><hl_color>Honorable Mention Award</hl_color> by Computational Visual Media Journal</strong>,
				<i>for "Specificity-preserving RGB-D saliency detection".</i>
				<a href="https://link.springer.com/journal/41095/updates/26965690" target="_blank">[Link]</a>
			</li> 

			<li>
				2023: <strong><hl_color>Achievement Award</hl_color> of 2023 TOP 100 Benchmarks & Evaluation by International Open Benchmark Council</strong>. <a href="./Awards/2023-ichallenge.pdf" target="_blank">[Certification]</a>
			</li>

			<li>
				2023: <strong><hl_color>Best Paper Award</hl_color> of Distributed, Collaborative and Federated Learning (DeCAF) Workshop in MICCAI</strong>,
				<i>for "Federated model aggregation via self-supervised priors for highly imbalanced medical image classification".</i>
				<a href="./Awards/2023-DeCaF_best_paper.pdf" target="_blank">[Certification]</a>
			</li>

			<li>
				2022: <strong><hl_color>Best Paper Award</hl_color> of Ophthalmic Medical Image Analysis (OMIA) Workshop in MICCAI</strong>, 
				<i>for "Localizing anatomical landmarks in ocular images using zoom-in attentive networks".</i>
				<a href="./Awards/2022-OMIA-best-oral.pdf" target="_blank">[Certification]</a>
			</li>
				
			<li>
				2022: <strong><hl_color>Best Paper Runner-up Award</hl_color> of Resource-Efficient Medical Image Analysis (REMIA) Workshop in MICCAI</strong>, 
				<i>for "Facing annotation redundancy: Oct layer segmentation with only 10 annotated pixels per layer".</i>
				<a href="./Awards/2022-Remia-Award.pdf" target="_blank">[Certification]</a>
			</li>

			<li>
				2021: <strong><hl_award>Best Paper Award</hl_award> in IEEE International Conference on Multimedia & Expo (ICME)</strong>,
				<i>for "Cross-view equivariant auto-encoder".</i> 
				<a href="http://2021.ieeeicme.org/2021.ieeeicme.org/best_paper_awards.html" target="_blank">[Link]</a>
				
			</li> 
			

			<!-- <li>
				2021: <strong><hl_color>Finalist</hl_color> of the Young Scientist Publication Impact Award in MICCAI</strong>,
				<i>for "DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field".</i>
				<a href="./Awards/2021-MICCAI-YSP-Finalist.pdf" target="_blank">[Certification]</a>
			</li> -->
				
			<li>
				2021: <strong><hl_color>Most Influential Paper (Application) Award</hl_color> in Jittor Developer Conference</strong>,
				 <i>for "PraNet: Parallel Reverse Attention Network for Polyp Segmentation".</i>
				<a href="./Awards/2021-Jitter-Award.pdf" target="_blank">[Certification in Chinese]</a>
			</li>  

			
			</ul>
			
			<div align="right">
				<a href="#top-page">[<b>Back to top</b>]</a>
			</div> 

			<hr />
			
			<br>

		</section>

	</div>
</body>

</html>
